{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scrapping overview \n",
    "\n",
    "Objective:\n",
    "\n",
    "1.  Some places to look for data\n",
    "2.  Downloading and parsing web pages\n",
    "3.  A couple of other libraries for \"scraping\" . . . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some places to look for data\n",
    "\n",
    "This is not an exhaustive list.\n",
    "\n",
    "* [Project Gutenberg](http://www.gutenberg.org/).  It should be possible to hunt up a library to search and download texts; for example, see https://github.com/c-w/gutenberg.\n",
    "\n",
    "* [Internet Archive](https://archive.org/).  [This python module](https://github.com/jjjake/internetarchive) works well. \n",
    "\n",
    "* [Folger Digital Texts](http://www.folgerdigitaltexts.org/download/).\n",
    "\n",
    "* [Oxford Text Archive](https://ota.ox.ac.uk/).\n",
    "\n",
    "* [eBooks@Adelaide](https://ebooks.adelaide.edu.au/).\n",
    "\n",
    "* We've made some additional corpora (Inagural Addresses, State of the Union Addresses) available on box.\n",
    "\n",
    "\n",
    "### Huge data . . . *not* for the end-of-semester project\n",
    "\n",
    "Neither is this.\n",
    "\n",
    "* [Common Crawl](http://commoncrawl.org/)\n",
    "\n",
    "* [Wikipedia](https://dumps.wikimedia.org/), of course.  But also [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page), and especially its much smaller (but still quite large) [database dumps](https://dumps.wikimedia.org/simplewiki/20171120/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading one web page\n",
    "\n",
    "Cribbing from [Programming Historian](https://programminghistorian.org/lessons/working-with-web-pages), again.  And from [the Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "Note that I'm not verifying the certificate (\"create_unverified_context\"), and that I'm calling a method that is not \"supposed\" to be used by code outside its module (the method name starts with an underscore).  Both of these things mark me as ethically challenged, since I've let expedience overrule broadly accepted norms around such things.\n",
    "\n",
    "Lots of people use [requests](http://docs.python-requests.org/en/master/) instead of urllib2.  Feel free to try it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2, ssl\n",
    "\n",
    "url = 'https://ebooks.adelaide.edu.au/meta/collections'\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "response = urllib2.urlopen(url, context=context)\n",
    "\n",
    "html = response.read()\n",
    "\n",
    "print html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the web page we just downloaded\n",
    "\n",
    "The variable **html** contains the downloaded web page. html is type string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "parsed_html = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "print 'type(html)', type(html)\n",
    "print\n",
    "print 'type(parsed_html)', type(parsed_html)\n",
    "print\n",
    "print parsed_html.prettify()[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the first and last 20 links\n",
    "\n",
    "Links are \"a\" tags.  It helps to know something about html.  [A quick tutorial](https://www.w3schools.com/html/) may help you along.  Also, see the doc for the [Chrome developer tools](https://developer.chrome.com/devtools) and [Firefox web developer](https://developer.mozilla.org/en-US/docs/Tools) tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_links = parsed_html.find_all('a')\n",
    "\n",
    "print\n",
    "for a in all_links[:20]:\n",
    "    print a\n",
    "    \n",
    "print\n",
    "for a in all_links[len(all_links) - 20:]:\n",
    "    print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictionary for categories and links\n",
    "\n",
    "In effect, we're extracting a crude catalog to the Adelaide ebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_links = parsed_html.find_all('a')\n",
    "\n",
    "categories_links = {}\n",
    "last_category = ''\n",
    "\n",
    "for a in all_links:\n",
    "    if a.get('class') != None and a.get('class')[0].startswith('mdi-menu') == True:\n",
    "        last_category = a.text\n",
    "        categories_links[last_category] = []\n",
    "    elif a.get('class') != None and a.get('class')[0].startswith('mdi-') == True:\n",
    "        break\n",
    "    elif last_category > '':\n",
    "        categories_links[last_category].append([a.get('href'), a.text])\n",
    "        \n",
    "#for category in sorted(categories_links.keys()):\n",
    "#    print\n",
    "#    print category\n",
    "#    for link in categories_links[category]:\n",
    "#        print '\\t', link\n",
    "\n",
    "print\n",
    "print sorted(categories_links.keys())\n",
    "print\n",
    "print categories_links['History--Ancient Greece']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab a bunch of texts, etc\n",
    "\n",
    "Use categories_links (i.e., our crude catalog to the Adelaide ebooks) to extract the text for one category. \n",
    "\n",
    "My assumption here is that I want just the text proper; I don't want title pages, front matter, back matter, standard link text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib2, ssl, codecs, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for link in categories_links['History--Ancient Greece']:\n",
    "    \n",
    "    print 'downloaded and parsing', link[1]\n",
    "\n",
    "    text_url = 'https://ebooks.adelaide.edu.au' + link[0] + 'complete.html'\n",
    "\n",
    "    text_context = ssl._create_unverified_context()\n",
    "    text_response = urllib2.urlopen(text_url, context=text_context)\n",
    "\n",
    "    text_html = text_response.read()\n",
    "    \n",
    "    text_parsed_html = BeautifulSoup(text_html, 'html.parser')\n",
    "    \n",
    "    extracted_text = []\n",
    "    \n",
    "    for div in text_parsed_html.find_all('div'):\n",
    "        \n",
    "        if div.parent.name == 'body':\n",
    "        \n",
    "            if div.get('id') != None and div.get('id') in ['controls', 'contents']:\n",
    "                pass\n",
    "            elif div.get('class') != None and \\\n",
    "                div.get('class')[0] in ['titlepage', \n",
    "                                        'titleverso', \n",
    "                                        'contents', \n",
    "                                        'frontmatter', \n",
    "                                        'preface', \n",
    "                                        'colophon',\n",
    "                                        'frontispiece',\n",
    "                                        'map']:\n",
    "                pass\n",
    "            else:\n",
    "                #print '\\t', 'extracting div', div.get('id'), div.get('class')\n",
    "                extracted_text.append(div.get_text())\n",
    "            \n",
    "    file_name = 'picky_' + re.sub('[^A-Z]', '_', link[1].upper()) + '.txt'\n",
    "    \n",
    "    f = codecs.open(file_name, 'w', encoding='utf-8')\n",
    "    f.write('\\n'.join(extracted_text))\n",
    "    f.close()\n",
    "    \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic bash (i.e., \"terminal\" or \"shell\") commands to check the results\n",
    "\n",
    "You might look at [our brief guide to the command line](https://talus.artsci.wustl.edu/command_line.html).\n",
    "\n",
    "If you're reading ahead, or you're bored, you might have a look at [Data Science at the Command Line](https://www.datascienceatthecommandline.com/).  I haven't looked at it yet; I plan to in May.\n",
    "\n",
    "**NOTE** that I extracted 0 characters for *The Calvary General*.  The text of *The Calvary General* is all in a div which has a frontmatter id and class; there's no way, short of expanding the logic to handle *The Calvary General* specially, to both keep frontmatter out of the other texts and include it in *The Calvary General*.  I.e., **a general rule for extracting content fails, even when I'm taking just one kind of document from what appears to be a well-designed web site.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -l *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wc -w *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 70 picky_ANABASIS___XENOPHON__TRANSLATED_BY_H__G__DAKYNS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the results include footnotes . . . \n",
    "\n",
    ". . . which would seem to be something, like prefatory material, that I would otherwise want to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!grep 'a district and town in the south-west of Arcadia' picky_ANABASIS___XENOPHON__TRANSLATED_BY_H__G__DAKYNS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Maybe I shouldn't be so fussy . . . \n",
    "\n",
    ". . . just take all the text in the body of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2, ssl, codecs, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for link in categories_links['History--Ancient Greece']:\n",
    "    \n",
    "    print 'downloaded and parsing', link[1]\n",
    "\n",
    "    text_url = 'https://ebooks.adelaide.edu.au' + link[0] + 'complete.html'\n",
    "\n",
    "    text_context = ssl._create_unverified_context()\n",
    "    text_response = urllib2.urlopen(text_url, context=text_context)\n",
    "\n",
    "    text_html = text_response.read()\n",
    "    \n",
    "    text_parsed_html = BeautifulSoup(text_html, 'html.parser')\n",
    "            \n",
    "    file_name = 'everything_' + re.sub('[^A-Z]', '_', link[1].upper()) + '.txt'\n",
    "    \n",
    "    f = codecs.open(file_name, 'w', encoding='utf-8')\n",
    "    f.write(text_parsed_html.body.get_text())\n",
    "    f.close()\n",
    "    \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the results\n",
    "\n",
    "Are the \"everything\" files good enough?  It depends on what I want to do with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -l *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wc -w *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 100 everything_ANABASIS___XENOPHON__TRANSLATED_BY_H__G__DAKYNS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML?\n",
    "\n",
    "We use [lxml](http://lxml.de/index.html) instead of BeautifulSoup to parse HTML (although lxml will parse html, and use BeautifulSoup to do it!).\n",
    "\n",
    "Note that I'm printing out only one speech for five speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "tree = etree.parse('http://www.folgerdigitaltexts.org/download/teisimple/Mac.xml')\n",
    "\n",
    "speakers = []\n",
    "for sp in tree.xpath('//tei:sp', namespaces={'tei': 'http://www.tei-c.org/ns/1.0'}):\n",
    "    speakers.append(sp.get('who'))\n",
    "    \n",
    "speakers = sorted(list(set(speakers)))\n",
    "\n",
    "print speakers\n",
    "\n",
    "stylesheet = etree.XML(\"\"\"<xsl:stylesheet version=\"1.0\" \n",
    "    xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\"\n",
    "    xmlns:tei=\"http://www.tei-c.org/ns/1.0\">\n",
    "    \n",
    "    <xsl:output method=\"text\" omit-xml-declaration=\"yes\" indent=\"no\"/>\n",
    "    \n",
    "    <xsl:template match=\"tei:speaker|tei:stage\"/>\n",
    "    \n",
    "    <xsl:template match=\"tei:sp/text()|tei:p/text()|tei:l/text()|tei:q/text()\"/>\n",
    "    \n",
    "    <xsl:template match=\"tei:w|tei:pc\"><xsl:apply-templates/></xsl:template>\n",
    "    <xsl:template match=\"tei:c\"><xsl:text> </xsl:text></xsl:template>\n",
    "    <xsl:template match=\"tei:lb\"><xsl:text> </xsl:text></xsl:template>\n",
    "    \n",
    "    <xsl:template match=\"tei:l|tei:q|tei:p\">\n",
    "        <xsl:apply-templates/>\n",
    "        <xsl:text>&#xa;</xsl:text>\n",
    "    </xsl:template>\n",
    "    \n",
    "    <xsl:template match=\"tei:q\">\n",
    "        <xsl:apply-templates/>\n",
    "        <xsl:text>&#xa;</xsl:text>\n",
    "        <xsl:text>&#xa;</xsl:text>\n",
    "    </xsl:template>\n",
    "    \n",
    "    <xsl:template match=\"/\">\n",
    "        <xsl:apply-templates/>\n",
    "    </xsl:template>\n",
    "    \n",
    "</xsl:stylesheet>\"\"\")\n",
    "\n",
    "transform = etree.XSLT(stylesheet)\n",
    "\n",
    "for speaker in speakers[:5]:\n",
    "                 \n",
    "    print\n",
    "    print '\\t', speaker\n",
    "    print\n",
    "    \n",
    "    for speech in tree.xpath('//tei:sp[@who=\"' + speaker + '\"]', namespaces={'tei': 'http://www.tei-c.org/ns/1.0'}):\n",
    "                             \n",
    "        result = transform(speech)\n",
    "                             \n",
    "        print unicode(result)\n",
    "        print\n",
    "        \n",
    "        break\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about [Scrapy](https://scrapy.org/)?\n",
    "\n",
    "Inconvenient in Jupyter Notebooks . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "class ScrapeAdelaideLinks(scrapy.Spider):\n",
    "    \n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://ebooks.adelaide.edu.au/meta/collections',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        \n",
    "        all_links = response.selector.xpath('//a')\n",
    "\n",
    "        categories_links = {}\n",
    "        last_category = ''\n",
    "\n",
    "        for a in all_links:\n",
    "            \n",
    "            node_class = None\n",
    "            if len(a.xpath(\"@class\").extract()) > 0:\n",
    "                node_class = a.xpath(\"@class\").extract()[0]\n",
    "            \n",
    "            node_href = None\n",
    "            if len(a.xpath(\"@href\").extract()) > 0:\n",
    "                node_href = a.xpath(\"@href\").extract()[0]\n",
    "            \n",
    "            if node_class != None and node_class.startswith('mdi-menu') == True:\n",
    "                last_category = a.xpath(\"./text()\").extract()[0]\n",
    "                categories_links[last_category] = []\n",
    "            elif node_class != None and node_class.startswith('mdi-') == True:\n",
    "                break\n",
    "            elif last_category > '':\n",
    "                categories_links[last_category].append([node_href, a.xpath(\"./text()\").extract()])\n",
    "        \n",
    "        print\n",
    "        print 'len(categories_links.keys())', len(categories_links.keys())\n",
    "        print\n",
    "        print categories_links.keys()[:20]\n",
    "        print\n",
    "        print categories_links['History--Ancient Greece']\n",
    "        \n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(ScrapeAdelaideLinks)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
